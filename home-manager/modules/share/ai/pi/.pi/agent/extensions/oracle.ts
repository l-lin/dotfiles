/**
 * Oracle Extension - Get a second opinion from another AI model
 *
 * Usage:
 *   /oracle <prompt>              - Opens model picker, then queries
 *   /oracle -m gpt-4o <prompt>    - Direct to specific model
 *   /oracle -f file.ts <prompt>   - Include file(s) in context
 *
 * Stay on your main model (e.g., Claude Opus) and get tie-breaker opinions!
 *
 * src: https://github.com/hjanuschka/shitty-extensions/blob/0898c8ce4bf4d598863e0d9a979485d533445f15/extensions/oracle.ts
 * Adapted with my own keymaps
 */

import { complete, type UserMessage, type Model } from "@mariozechner/pi-ai";
import type {
  ExtensionAPI,
  ExtensionContext,
  SessionEntry,
} from "@mariozechner/pi-coding-agent";
import {
  BorderedLoader,
  convertToLlm,
  serializeConversation,
} from "@mariozechner/pi-coding-agent";
import { Text, Key, matchesKey, visibleWidth } from "@mariozechner/pi-tui";
import * as fs from "node:fs";
import * as path from "node:path";

// Models to try - provider and model ID as used by pi's modelRegistry.find()
const ORACLE_MODELS = [
  // Github Copilot
  { provider: "github-copilot", model: "gpt-4o", name: "GPT-4o" },
  { provider: "github-copilot", model: "gpt-4.1", name: "GPT-4.1" },
  { provider: "github-copilot", model: "gpt-5-mini", name: "GPT-5 Mini" },
  { provider: "openai", model: "gpt-4.1-mini", name: "GPT-4.1 Mini" },
  { provider: "openai", model: "gpt-4.1-nano", name: "GPT-4.1 Nano" },
  { provider: "openai", model: "o1", name: "o1" },
  { provider: "openai", model: "o1-mini", name: "o1-mini" },
  { provider: "openai", model: "o1-pro", name: "o1-pro" },
  { provider: "openai", model: "o3-mini", name: "o3-mini" },
  // Anthropic
  { provider: "anthropic", model: "claude-haiku-3-5", name: "Claude Haiku 3.5" },
  { provider: "anthropic", model: "claude-sonnet-4-5", name: "Claude Sonnet 4.5" },
  { provider: "anthropic", model: "claude-opus-4-5", name: "Claude Opus 4.5" },
  { provider: "anthropic", model: "claude-opus-4-6", name: "Claude Opus 4.6" },
] as const;

interface AvailableModel {
  provider: string;
  modelId: string;
  name: string;
  model: Model;
  apiKey: string;
}

// ANSI helpers
const dim = (s: string) => `\x1b[2m${s}\x1b[22m`;
const bold = (s: string) => `\x1b[1m${s}\x1b[22m`;
const green = (s: string) => `\x1b[32m${s}\x1b[0m`;
const yellow = (s: string) => `\x1b[33m${s}\x1b[0m`;
const cyan = (s: string) => `\x1b[36m${s}\x1b[0m`;
const magenta = (s: string) => `\x1b[35m${s}\x1b[0m`;

/**
 * Oracle result display with add to context option
 */
class OracleResultComponent {
  private result: string;
  private modelName: string;
  private prompt: string;
  private selected: number = 0; // 0 = Yes, 1 = No
  private scrollOffset: number = 0;
  private onDone: (addToContext: boolean) => void;
  private tui: { requestRender: () => void };
  private cachedLines: string[] = [];
  private cachedWidth = 0;

  constructor(
    result: string,
    modelName: string,
    prompt: string,
    tui: { requestRender: () => void },
    onDone: (addToContext: boolean) => void,
  ) {
    this.result = result;
    this.modelName = modelName;
    this.prompt = prompt;
    this.tui = tui;
    this.onDone = onDone;
  }

  handleInput(data: string): void {
    if (matchesKey(data, "escape") || data === "n" || data === "N") {
      this.onDone(false);
      return;
    }

    if (matchesKey(data, "return") || matchesKey(data, "enter")) {
      this.onDone(this.selected === 0);
      return;
    }

    if (data === "y" || data === "Y") {
      this.onDone(true);
      return;
    }

    if (
      matchesKey(data, "left") ||
      matchesKey(data, "right") ||
      data === "h" ||
      data === "l" ||
      matchesKey(data, "tab")
    ) {
      this.selected = this.selected === 0 ? 1 : 0;
      this.cachedWidth = 0;
      this.tui.requestRender();
    }

    // Scroll through result
    if (matchesKey(data, "up") || data === "k" || matchesKey(data, Key.ctrl("p"))) {
      this.scrollOffset = Math.max(0, this.scrollOffset - 1);
      this.cachedWidth = 0;
      this.tui.requestRender();
    } else if (matchesKey(data, "down") || data === "j" || matchesKey(data, Key.ctrl("n"))) {
      this.scrollOffset++;
      this.cachedWidth = 0;
      this.tui.requestRender();
    }
  }

  invalidate(): void {
    this.cachedWidth = 0;
  }

  render(width: number): string[] {
    if (this.cachedWidth === width) {
      return this.cachedLines;
    }

    const lines: string[] = [];
    const boxWidth = Math.min(80, width - 4);
    const contentWidth = boxWidth - 4;
    const maxResultLines = 15;

    const padLine = (line: string): string => {
      const len = visibleWidth(line);
      return line + " ".repeat(Math.max(0, width - len));
    };

    const boxLine = (content: string): string => {
      const len = visibleWidth(content);
      const padding = Math.max(0, boxWidth - 2 - len);
      return dim("â”‚ ") + content + " ".repeat(padding) + dim(" â”‚");
    };

    // Wrap text to fit in box
    const wrapText = (text: string, maxWidth: number): string[] => {
      const wrapped: string[] = [];
      for (const paragraph of text.split("\n")) {
        if (paragraph.length <= maxWidth) {
          wrapped.push(paragraph);
        } else {
          let remaining = paragraph;
          while (remaining.length > maxWidth) {
            let breakPoint = remaining.lastIndexOf(" ", maxWidth);
            if (breakPoint === -1) breakPoint = maxWidth;
            wrapped.push(remaining.slice(0, breakPoint));
            remaining = remaining.slice(breakPoint + 1);
          }
          if (remaining) wrapped.push(remaining);
        }
      }
      return wrapped;
    };

    lines.push("");
    lines.push(padLine(dim("â•­" + "â”€".repeat(boxWidth) + "â•®")));
    lines.push(
      padLine(boxLine(bold(magenta(`ðŸ”® Oracle Response (${this.modelName})`)))),
    );
    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));

    // Show prompt
    const promptPreview =
      this.prompt.length > contentWidth - 10
        ? this.prompt.slice(0, contentWidth - 13) + "..."
        : this.prompt;
    lines.push(padLine(boxLine(dim("Q: ") + promptPreview)));
    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));

    // Show result with scrolling
    const resultLines = wrapText(this.result, contentWidth);
    const visibleLines = resultLines.slice(
      this.scrollOffset,
      this.scrollOffset + maxResultLines,
    );

    for (const line of visibleLines) {
      lines.push(padLine(boxLine(line)));
    }

    // Padding if result is short
    for (let i = visibleLines.length; i < Math.min(maxResultLines, 5); i++) {
      lines.push(padLine(boxLine("")));
    }

    // Scroll indicator
    if (resultLines.length > maxResultLines) {
      const scrollInfo = dim(
        ` â†‘â†“ scroll (${this.scrollOffset + 1}-${Math.min(this.scrollOffset + maxResultLines, resultLines.length)}/${resultLines.length})`,
      );
      lines.push(padLine(boxLine(scrollInfo)));
    }

    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));

    // Add to context prompt
    lines.push(padLine(boxLine(bold("Add to current conversation context?"))));
    lines.push(padLine(boxLine("")));

    // Buttons
    const yesBtn =
      this.selected === 0 ? green(bold(" [ YES ] ")) : dim("   YES   ");
    const noBtn =
      this.selected === 1 ? yellow(bold(" [ NO ] ")) : dim("   NO   ");

    const buttonLine = `       ${yesBtn}          ${noBtn}`;
    lines.push(padLine(boxLine(buttonLine)));
    lines.push(padLine(boxLine("")));

    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));
    lines.push(
      padLine(
        boxLine(
          dim("â†â†’/Tab") +
            " switch  " +
            dim("Enter") +
            " confirm  " +
            dim("Y/N") +
            " quick",
        ),
      ),
    );
    lines.push(padLine(dim("â•°" + "â”€".repeat(boxWidth) + "â•¯")));
    lines.push("");

    this.cachedLines = lines;
    this.cachedWidth = width;
    return lines;
  }
}

/**
 * Simple model picker component
 */
class ModelPickerComponent {
  private models: AvailableModel[];
  private selected: number = 0;
  private prompt: string;
  private files: string[];
  private onSelect: (model: AvailableModel) => void;
  private onCancel: () => void;
  private tui: { requestRender: () => void };
  private cachedLines: string[] = [];
  private cachedWidth = 0;

  constructor(
    models: AvailableModel[],
    prompt: string,
    files: string[],
    tui: { requestRender: () => void },
    onSelect: (model: AvailableModel) => void,
    onCancel: () => void,
  ) {
    this.models = models;
    this.prompt = prompt;
    this.files = files;
    this.tui = tui;
    this.onSelect = onSelect;
    this.onCancel = onCancel;
  }

  handleInput(data: string): void {
    if (matchesKey(data, "escape") || data === "q" || data === "Q") {
      this.onCancel();
      return;
    }

    if (matchesKey(data, "up") || data === "k" || matchesKey(data, Key.ctrl("p"))) {
      this.selected = Math.max(0, this.selected - 1);
      this.cachedWidth = 0;
      this.tui.requestRender();
    } else if (matchesKey(data, "down") || data === "j" || matchesKey(data, Key.ctrl("n"))) {
      this.selected = Math.min(this.models.length - 1, this.selected + 1);
      this.cachedWidth = 0;
      this.tui.requestRender();
    } else if (matchesKey(data, "return") || matchesKey(data, "enter")) {
      this.onSelect(this.models[this.selected]);
    } else if (data >= "1" && data <= "9") {
      const idx = parseInt(data) - 1;
      if (idx < this.models.length) {
        this.onSelect(this.models[idx]);
      }
    }
  }

  invalidate(): void {
    this.cachedWidth = 0;
  }

  render(width: number): string[] {
    if (this.cachedWidth === width) {
      return this.cachedLines;
    }

    const lines: string[] = [];
    const boxWidth = Math.min(60, width - 4);

    const padLine = (line: string): string => {
      const len = visibleWidth(line);
      return line + " ".repeat(Math.max(0, width - len));
    };

    const boxLine = (content: string): string => {
      const len = visibleWidth(content);
      const padding = Math.max(0, boxWidth - 2 - len);
      return dim("â”‚ ") + content + " ".repeat(padding) + dim(" â”‚");
    };

    lines.push("");
    lines.push(padLine(dim("â•­" + "â”€".repeat(boxWidth) + "â•®")));
    lines.push(padLine(boxLine(bold(magenta("ðŸ”® Oracle - Second Opinion")))));
    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));

    // Prompt preview
    const maxPromptLen = boxWidth - 12;
    const promptPreview =
      this.prompt.length > maxPromptLen
        ? this.prompt.slice(0, maxPromptLen - 3) + "..."
        : this.prompt;
    lines.push(padLine(boxLine(dim("Prompt: ") + promptPreview)));

    // Files
    if (this.files.length > 0) {
      const filesStr = this.files
        .map((f) => cyan("@" + path.basename(f)))
        .join(" ");
      lines.push(padLine(boxLine(dim("Files:  ") + filesStr)));
    }

    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));
    lines.push(
      padLine(boxLine(dim("â†‘â†“/jk navigate â€¢ 1-9 quick select â€¢ Enter send"))),
    );
    lines.push(padLine(boxLine("")));

    // Model list
    for (let i = 0; i < this.models.length; i++) {
      const m = this.models[i];
      const num = i < 9 ? yellow(`${i + 1}`) : " ";
      const pointer = i === this.selected ? green("â¯ ") : "  ";
      const name = i === this.selected ? green(bold(m.name)) : m.name;
      const provider = dim(` (${m.provider})`);
      lines.push(padLine(boxLine(`${pointer}${num}. ${name}${provider}`)));
    }

    lines.push(padLine(boxLine("")));
    lines.push(padLine(dim("â”œ" + "â”€".repeat(boxWidth) + "â”¤")));
    lines.push(padLine(boxLine(dim("Esc") + " cancel")));
    lines.push(padLine(dim("â•°" + "â”€".repeat(boxWidth) + "â•¯")));
    lines.push("");

    this.cachedLines = lines;
    this.cachedWidth = width;
    return lines;
  }
}

export default function (pi: ExtensionAPI) {
  pi.registerCommand("oracle", {
    description: "Get a second opinion from another AI model",
    handler: async (args, ctx) => {
      if (!ctx.hasUI) {
        ctx.ui.notify("oracle requires interactive mode", "error");
        return;
      }

      // Find available models (with API keys, excluding current)
      const availableModels: AvailableModel[] = [];

      for (const m of ORACLE_MODELS) {
        const model = ctx.modelRegistry.find(m.provider, m.model);
        if (!model) continue;

        // Skip current model - we want a DIFFERENT opinion
        if (ctx.model && model.id === ctx.model.id) continue;

        const apiKey = await ctx.modelRegistry.getApiKey(model);
        if (!apiKey) continue;

        availableModels.push({
          provider: m.provider,
          modelId: m.model,
          name: m.name,
          model,
          apiKey,
        });
      }

      if (availableModels.length === 0) {
        ctx.ui.notify(
          "No alternative models available. Check API keys.",
          "error",
        );
        return;
      }

      // Parse args
      const trimmedArgs = args?.trim() || "";
      if (!trimmedArgs) {
        ctx.ui.notify(
          "Usage: /oracle <prompt> or /oracle -f file.ts <prompt>",
          "error",
        );
        return;
      }

      let modelArg: string | undefined;
      const files: string[] = [];
      const promptParts: string[] = [];

      const tokens = trimmedArgs.split(/\s+/);
      let i = 0;
      while (i < tokens.length) {
        const token = tokens[i];
        if (token === "-m" || token === "--model") {
          i++;
          if (i < tokens.length) modelArg = tokens[i];
        } else if (token === "-f" || token === "--file") {
          i++;
          if (i < tokens.length) files.push(tokens[i]);
        } else {
          promptParts.push(...tokens.slice(i));
          break;
        }
        i++;
      }

      const prompt = promptParts.join(" ");
      if (!prompt) {
        ctx.ui.notify("No prompt provided", "error");
        return;
      }

      // If model specified directly, skip picker
      if (modelArg) {
        const found = availableModels.find(
          (m) =>
            m.modelId === modelArg ||
            m.modelId.includes(modelArg!) ||
            m.name.toLowerCase().includes(modelArg!.toLowerCase()),
        );
        if (!found) {
          ctx.ui.notify(`Model "${modelArg}" not available`, "error");
          return;
        }
        await executeOracle(pi, ctx, prompt, files, found);
        return;
      }

      // Show model picker
      const selectedModel = await ctx.ui.custom<AvailableModel | null>(
        (tui, _theme, _kb, done) => {
          const picker = new ModelPickerComponent(
            availableModels,
            prompt,
            files,
            tui,
            (model) => done(model),
            () => done(null),
          );

          return {
            render: (w) => picker.render(w),
            invalidate: () => picker.invalidate(),
            handleInput: (data) => picker.handleInput(data),
          };
        },
      );

      if (!selectedModel) {
        ctx.ui.notify("Cancelled", "info");
        return;
      }

      await executeOracle(pi, ctx, prompt, files, selectedModel);
    },
  });

  // Custom renderer for oracle responses
  pi.registerMessageRenderer("oracle-response", (message, options, theme) => {
    const { expanded } = options;
    const details = message.details || {};

    let text = theme.fg(
      "accent",
      `ðŸ”® Oracle (${details.modelName || "unknown"}):\n\n`,
    );
    text += message.content;

    if (expanded && details.files?.length > 0) {
      text += "\n\n" + theme.fg("dim", `Files: ${details.files.join(", ")}`);
    }

    return new Text(text, 0, 0);
  });
}

async function executeOracle(
  pi: ExtensionAPI,
  ctx: ExtensionContext,
  prompt: string,
  files: string[],
  model: AvailableModel,
): Promise<void> {
  // Get conversation context from current session
  const branch = ctx.sessionManager.getBranch();
  const messages = branch
    .filter(
      (entry): entry is SessionEntry & { type: "message" } =>
        entry.type === "message",
    )
    .map((entry) => entry.message);

  let conversationContext = "";
  if (messages.length > 0) {
    const llmMessages = convertToLlm(messages);
    conversationContext = serializeConversation(llmMessages);
  }

  // Build context from files
  let fileContext = "";
  for (const file of files) {
    try {
      const fullPath = path.resolve(ctx.cwd, file);
      const content = fs.readFileSync(fullPath, "utf-8");
      fileContext += `\n\n--- File: ${file} ---\n${content}`;
    } catch (err) {
      fileContext += `\n\n--- File: ${file} ---\n[Error reading file: ${err}]`;
    }
  }

  // Build full prompt with conversation context
  let fullPrompt = "";
  if (conversationContext) {
    fullPrompt += `## Current Conversation Context\n\n${conversationContext}\n\n`;
  }
  fullPrompt += `## Question for Second Opinion\n\n${prompt}`;
  if (fileContext) {
    fullPrompt += `\n\n## Additional Files${fileContext}`;
  }

  // Call the model
  const result = await ctx.ui.custom<string | null>((tui, theme, _kb, done) => {
    const loader = new BorderedLoader(tui, theme, `ðŸ”® Asking ${model.name}...`);
    loader.onAbort = () => done(null);

    const doQuery = async () => {
      const userMessage: UserMessage = {
        role: "user",
        content: [{ type: "text", text: fullPrompt }],
        timestamp: Date.now(),
      };

      const response = await complete(
        model.model,
        {
          systemPrompt: `You are providing a second opinion on a coding conversation. 
You have access to the full conversation context between the user and their primary AI assistant.
Your job is to:
1. Understand what they've been discussing
2. Answer the specific question they're asking you
3. Point out if you disagree with any decisions made
4. Be concise but thorough

Focus on being helpful and providing a fresh perspective.`,
          messages: [userMessage],
        },
        { apiKey: model.apiKey, signal: loader.signal },
      );

      if (response.stopReason === "aborted") {
        return null;
      }

      return response.content
        .filter((c): c is { type: "text"; text: string } => c.type === "text")
        .map((c) => c.text)
        .join("\n");
    };

    doQuery()
      .then(done)
      .catch((err) => {
        console.error("Oracle error:", err);
        done(null);
      });

    return loader;
  });

  if (result === null) {
    ctx.ui.notify("Cancelled or failed", "warning");
    return;
  }

  // Show result and ask if user wants to add to context
  const addToContext = await ctx.ui.custom<boolean>(
    (tui, _theme, _kb, done) => {
      const component = new OracleResultComponent(
        result,
        model.name,
        prompt,
        tui,
        (add) => done(add),
      );

      return {
        render: (w) => component.render(w),
        invalidate: () => component.invalidate(),
        handleInput: (data) => component.handleInput(data),
      };
    },
  );

  if (addToContext) {
    // Add Oracle's response to the conversation
    pi.sendMessage({
      customType: "oracle-response",
      content: result,
      display: true,
      details: {
        model: model.modelId,
        modelName: model.name,
        files,
        prompt,
      },
    });
    ctx.ui.notify(`Oracle response added to context`, "success");
  } else {
    ctx.ui.notify(`Oracle response discarded`, "info");
  }
}
